{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Laboratorio di introduzione all'apprendimento automatico - Lezione 2**\n"
      ],
      "metadata": {
        "id": "JMXwixJ_Smxw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lezione 2**\n",
        "\n",
        "Argomenti della lezione odierna:\n",
        "\n",
        "1.  Skitlearn.\n",
        "2.  Apprendimento supervisionato.\n",
        "3.  Caso di studio, il tasks di classificazione.\n"
      ],
      "metadata": {
        "id": "T_vtmNx7gmvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tipi di Apprendimento\n",
        "\n",
        "**Supervised Learning:**\n",
        "\n",
        "I dati in training e test hanno entrambi dati etichettati.\n",
        "\n",
        "$$\n",
        "\\text{Training:}\n",
        "$$\n",
        "\n",
        "$$\n",
        "[(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)] \\text{ dove } (x_i \\in X, y_i \\in Y)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Testing:}\n",
        "$$\n",
        "\n",
        "$$\n",
        "[(x'_1, y'_1), (x'_2, y'_2), \\ldots, (x'_m, y'_m)] \\text{ dove } (x'_j \\in X, y'_j \\in Y)\n",
        "$$\n",
        "\n",
        "\n",
        "**Unsupervised Learning:**\n",
        "\n",
        "I dati in training e test non hanno dati etichettati.\n",
        "\n",
        "$$\n",
        "\\text{Training:}\n",
        "$$\n",
        "\n",
        "$$\n",
        "[ (x_1),( x_2), \\ldots, (x_n)] \\text{ dove } (x_i \\in X)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Testing: }\n",
        "$$\n",
        "\n",
        "$$\n",
        "[(x'_1), (x'_2), \\ldots, (x'_m)] \\text{ dove } (x'_j \\in X)\n",
        "$$\n",
        "\n",
        "\n",
        "**Semi-Supervised Learning:**\n",
        "\n",
        "I dati in training possono avere degli esempi etichettati, in test no.\n",
        "\n",
        "$$\n",
        "\\text{Training: }\n",
        "$$\n",
        "\n",
        "$$\n",
        "[(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)] \\text{ dove } (x_i \\in X, y_i \\in Y)\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{Testing: }\n",
        "$$\n",
        "\n",
        "$$\n",
        "[(x'_1), (x'_2), \\ldots, (x'_m)] \\text{ dove } (x'_j \\in X)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FKzHmHEXmgFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supervised Learning Tasks:\n",
        "\n",
        "*   Classificazione\n",
        "\n",
        "*   Regressione\n",
        "\n",
        "\n",
        "Unsupervised Learning Tasks:\n",
        "\n",
        "*  Clustering\n",
        "\n",
        "*  Riduzione della dimensionalità\n",
        "\n",
        "*  Outlier Detection\n",
        "\n",
        "*  Estrapolazione di features nascoste\n",
        "\n"
      ],
      "metadata": {
        "id": "xCd5zZa50GIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pandas e Skitlearn"
      ],
      "metadata": {
        "id": "qfiBDQVySJXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pandas\n",
        "\n",
        "Pandas fornisce strutture dati di alto livello come DataFrame e Series, che sono più flessibili e facili da usare rispetto agli array NumPy. Queste strutture rendono più semplice organizzare e analizzare dati tabellari."
      ],
      "metadata": {
        "id": "l9aZIo_qDfiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "mydataset = {\n",
        "  'names': np.array(['Alice', 'Bob', 'Charlie', 'David', 'Eve']),\n",
        "  'ages':  np.array([20, 22, 21, 23, 20]),\n",
        "  'scores':np.array([85, 92, 78, 95, 88])\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(mydataset)\n",
        "df"
      ],
      "metadata": {
        "id": "uh6bN3VHDhMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataframes pandas sono strutture bidimensionali che contengono una o piu colonne.\n",
        "Le colonne a loro volta sono Series, ovvero oggetti unidimensionali."
      ],
      "metadata": {
        "id": "cHCWxUFURS2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print( f\"DataFrame:\\t\\t{type(df)}\")\n",
        "print( f\"DataFrame[ages]:\\t{type(df['ages']) }\")"
      ],
      "metadata": {
        "id": "8NorjGa0RR3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"ages\"] + 3\n"
      ],
      "metadata": {
        "id": "l_b05m6zSioM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = df[\"ages\"] + df[\"scores\"]\n",
        "res"
      ],
      "metadata": {
        "id": "gB52bj9USuS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = df[ df[\"names\"] == \"David\"]\n",
        "res"
      ],
      "metadata": {
        "id": "mL_3iVglI3Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = df[ df[\"scores\"] > 90 ]\n",
        "res"
      ],
      "metadata": {
        "id": "1_TRxfWPI9oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = df[(df[\"scores\"] > 80) & (df[\"scores\"] < 90)]\n",
        "res"
      ],
      "metadata": {
        "id": "wzaWvIIGJPR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nuova colonna\n",
        "np.random.seed(42)\n",
        "df[\"nuova\"] = np.random.randint(0,10, df.shape[0])\n",
        "df"
      ],
      "metadata": {
        "id": "Nz_GkRLJQweI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas fornisce metodi utili per l'esportazione e l'acquisizione dei dati"
      ],
      "metadata": {
        "id": "6A5iJVLMJfWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#come file path potete mettere direttamente quello del vostro file in locale.\n",
        "file_path_csv=\"test_csv.csv\"\n",
        "file_path_xls=\"test_xlsx.xlsx\"\n",
        "\n",
        "#I comandi di seguito creano e sovrascrivono i file se già esistono.\n",
        "#Se index uguale a false indica che non viene stampato l'indice di riga.\n",
        "df.to_csv(file_path_csv, index=False)\n",
        "df.to_excel(file_path_xls, index=False)\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv(file_path_csv)\n",
        "df = pd.read_excel(file_path_xls)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hfvc3dj8PlsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inoltre mette a disposizioni una serie di utiliti per preprocessare i dati. Faremo vedere alcuni esempi di seguito\n"
      ],
      "metadata": {
        "id": "MbiHm8MYbW0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creazione di un DataFrame fittizio con errori\n",
        "data = {\n",
        "    'Nome': ['Alice', 'Bob', 'Charlie', 'David', np.nan, 'Eve'],\n",
        "    'Età': [25, 30, 22, np.nan, 35, 28],\n",
        "    'Punteggio': [85, 92, 78, 64, 99, 88],\n",
        "    'Sesso': ['F', 'M', 'M', 'M', 'M', 'F'],\n",
        "    'Indirizzo': ['123 Main St', '456 Elm St', np.nan, '789 Oak St', '555 Pine St', '999 Maple St'],\n",
        "    'Nazionalità': ['USA', 'Canada', 'UK', 'USA', 'Germany', 'France']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Visualizzazione del DataFrame con errori\n",
        "print(\"DataFrame con errori:\")\n",
        "print(df)\n",
        "\n",
        "# Pulizia dei dati\n",
        "# 1. Rimozione delle righe con valori mancanti\n",
        "df = df.dropna()\n",
        "\n",
        "# 2. Rimozione dei duplicati\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# 3. Trasformazione dei valori in modo coerente (ad esempio, età in interi)\n",
        "df['Età'] = df['Età'].astype(int)\n",
        "\n",
        "# 4. Validazione dei dati (ad esempio, verificare che l'età sia compresa tra 18 e 99)\n",
        "df = df[(df['Età'] >= 18) & (df['Età'] <= 99)]\n",
        "\n",
        "# 5. Correzione di errori specifici (ad esempio, correggere un valore errato nell'indirizzo)\n",
        "df.loc[df['Nome'] == 'David', 'Indirizzo'] = '567 Cedar St'\n",
        "\n",
        "# 6. Trasformare M ed F in booleani True o false\n",
        "df['Donna'] = df['Sesso'] == 'F'  # True se 'M', False se 'F'\n",
        "\n",
        "# 7. Categorizzare automaticamente le nazionalità\n",
        "df['Nazionalità'] = df['Nazionalità'].astype('category').cat.codes\n",
        "\n",
        "# Visualizzazione del DataFrame pulito\n",
        "print(\"\\nDataFrame pulito:\")\n",
        "print(df)"
      ],
      "metadata": {
        "id": "g8JOcvhbbliv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SkitLearn\n",
        "\n",
        "E' una libreria pyton che contiene un insieme di tools per fare Machine Learning in python.\n",
        "\n",
        "Skitlean fornisce i metodi per implementare le fasi di sviluppo di un modello di apprendimento automatico\n",
        "\n",
        "*   Built-in datasets.\n",
        "*   Data preprocessing.\n",
        "*   Training dei modelli predittivi.\n",
        "*   Valutazione dei modelli"
      ],
      "metadata": {
        "id": "RXp0jyJofaP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn"
      ],
      "metadata": {
        "id": "rXysg_fmg96p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset Acquisitions\n",
        "I dataset di sklearn si dividono in 3 interfacce:\n",
        "\n",
        "*   **Dataset loaders**: per caricare piccoli dataset standard.\n",
        "\n",
        "*   **Dataset generation functions**: permettono di generare dei dataset sintetici.\n",
        "\n",
        "*   **Dataset fetchers**: permettono di scaricare grossi dataset open-source.\n"
      ],
      "metadata": {
        "id": "Iv3YSf2MffPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Dataset Loaders interface"
      ],
      "metadata": {
        "id": "DSYMJXkvmL-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scopo di esempio faremo vedere come è possibile importare il dataset IRIS.\n",
        "[doc link](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris)\n",
        "\n",
        "'Iris dataset è un dataset di esempio che contiene misurazioni di quattro caratteristiche di tre diverse specie di fiori Iris: Setosa, Versicolor e Virginica. Le quattro caratteristiche sono la lunghezza e la larghezza del sepalo e del petalo, misurate in centimetri.\n",
        "\n",
        "Questo dataset è ampiamente utilizzato per scopi educativi e dimostrativi poiché rappresenta un problema di classificazione multi-classe. L'obiettivo è di addestrare un modello di machine learning per prevedere la specie di un fiore Iris in base alle sue caratteristiche."
      ],
      "metadata": {
        "id": "9uYAxGSE8sDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://drive.google.com/uc?id=1LWoWWLUxUvzbW2CPX916SsXnlNYn-zOU )"
      ],
      "metadata": {
        "id": "JSNoTgxhhetM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris_dataset_np = load_iris() #ritorna un oggetto che contiene due numpy\n",
        "x_np = iris_dataset_np.data #x è di tipo numpy\n",
        "y_np = iris_dataset_np.target# y è di tipo numpy\n",
        "\n",
        "iris_dataset_pd = load_iris(as_frame=True) #as_frame torna un oggetto che contiene\n",
        "x_pd = iris_dataset_pd.data #è un DataFrame pandas\n",
        "y_pd = iris_dataset_pd.target #è un Series pandas\n",
        "\n",
        "x_pd"
      ],
      "metadata": {
        "id": "VXI3iVDuktDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iris Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "0aukjcp0iHIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = x_pd.copy()\n",
        "df[\"Targets\"]= y_pd\n",
        "df.head(10)#stampa le prime 10 righe"
      ],
      "metadata": {
        "id": "auiSFy3YiJWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vogliamo raggruppare i dati per avere delle statistiche descrittive"
      ],
      "metadata": {
        "id": "qn_SiVRAnecS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe() #computa le statistiche sui dati"
      ],
      "metadata": {
        "id": "TDwVUgitnDqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proviamo a mostrare le distribuzioni dei dati sul grafo."
      ],
      "metadata": {
        "id": "yL_mCztlnh2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "map_dict = {0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'}\n",
        "df['Species'] = df['Targets'].map(map_dict)\n",
        "\n",
        "def scatter_plot(df, x_name, y_name ):\n",
        "    for species, gruppo in df.groupby(\"Species\"):\n",
        "      plt.scatter(gruppo[x_name], gruppo[y_name], label=species )\n",
        "\n",
        "    # Mostra il grafico\n",
        "    plt.legend()\n",
        "    plt.xlabel(x_name)\n",
        "    plt.ylabel(y_name)\n",
        "    plt.show()\n",
        "\n",
        "scatter_plot( df, x_name=\"petal length (cm)\", y_name=\"petal width (cm)\")"
      ],
      "metadata": {
        "id": "z4I9bQTwnTAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vediamo la distribuzione dei dati rispetto alla lunghezza del sepalo"
      ],
      "metadata": {
        "id": "TwNBGbxGrjGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scatter_plot( df, x_name=\"sepal length (cm)\", y_name=\"sepal width (cm)\")"
      ],
      "metadata": {
        "id": "qdNeF93WqdFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df, hue=\"Targets\", palette=\"Set1\")"
      ],
      "metadata": {
        "id": "CHUFHBC6zRE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#correlazioni lineari con il coefficiente di pearson\n",
        "# Calcolo della correlazione con il coefficiente di Pearson\n",
        "correlation_matrix = df[df.columns[:-2]].corr(method='pearson')\n",
        "\n",
        "# Creazione di una mappa di calore\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
        "plt.title('Mappa di calore delle correlazioni (Coefficiente di Pearson)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6UtO2cXhkvEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Dataset Generation Functions interface"
      ],
      "metadata": {
        "id": "-rp3CRShzpVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs, make_moons, make_s_curve, make_circles\n",
        "\n",
        "n_samples = 500\n",
        "n_features = 2\n",
        "random_seed = 42\n",
        "noise_factor = 0.1\n",
        "\n",
        "# Genera il dataset con make_blobs\n",
        "X_blobs, y_blobs = make_blobs(n_samples=n_samples, n_features=n_features, centers=3, random_state=random_seed )\n",
        "\n",
        "# Genera il dataset con make_moons\n",
        "X_moons, y_moons = make_moons(n_samples=n_samples, noise=noise_factor, random_state=random_seed)\n",
        "\n",
        "# Genera il dataset con make_s_curve\n",
        "X_s_curve, y_s_curve = make_s_curve(n_samples=n_samples, noise=noise_factor, random_state=random_seed)\n",
        "\n",
        "# Genera il dataset con make_circles\n",
        "X_circles, y_circles = make_circles(n_samples=n_samples, noise=noise_factor, factor=0.1, random_state=random_seed)\n",
        "\n",
        "# Creazione del grafico\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "fig.suptitle('Visualizzazione dei Dataset', fontsize=16)\n",
        "\n",
        "# Plot per make_blobs\n",
        "axes[0, 0].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs, cmap='Set3')\n",
        "axes[0, 0].set_xlabel(\"Feature 1\")\n",
        "axes[0, 0].set_ylabel(\"Feature 2\")\n",
        "axes[0, 0].set_title(\"Dataset generato con make_blobs\")\n",
        "\n",
        "# Plot per make_moons\n",
        "axes[0, 1].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='Set1')\n",
        "axes[0, 1].set_xlabel(\"Feature 1\")\n",
        "axes[0, 1].set_ylabel(\"Feature 2\")\n",
        "axes[0, 1].set_title(\"Dataset generato con make_moons\")\n",
        "\n",
        "# Plot per make_s_curve\n",
        "axes[1, 0].scatter(X_s_curve[:, 0], X_s_curve[:, 2], c=y_s_curve, cmap='Set1')\n",
        "axes[1, 0].set_xlabel(\"Feature 1\")\n",
        "axes[1, 0].set_ylabel(\"Feature 3\")\n",
        "axes[1, 0].set_title(\"Dataset generato con make_s_curve\")\n",
        "\n",
        "# Plot per make_circles\n",
        "axes[1, 1].scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap='Set1')\n",
        "axes[1, 1].set_xlabel(\"Feature 1\")\n",
        "axes[1, 1].set_ylabel(\"Feature 2\")\n",
        "axes[1, 1].set_title(\"Dataset generato con make_circles\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "N9R35eRozxd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Dataset Fetchers"
      ],
      "metadata": {
        "id": "Zo5p05UVztmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The Labeled Faces in the Wild face recognition dataset\n",
        "[vai a vedere tutti i dataset](https://scikit-learn.org/stable/datasets/real_world.html)"
      ],
      "metadata": {
        "id": "SUYWjY9snbha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_lfw_people, fetch_covtype\n",
        "data2 = fetch_lfw_people(color=True) #false ritorna in scala di grigi\n",
        "# Estrai le immagini e le etichette\n",
        "X = data2.images\n",
        "y = data2.target\n",
        "\n",
        "# Stampare le dimensioni dei dati\n",
        "print(f'Dimensioni dei dati: {X.shape}')\n",
        "print(f'Dimensioni delle etichette: {y.shape}')\n"
      ],
      "metadata": {
        "id": "dl-vDt8RzyWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X[5])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F7nYzQV6yb7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Img shape{X[0].shape}\")\n",
        "print(f\"Min: {np.min(X)}\")\n",
        "print(f\"Max: {np.max(X)}\")\n",
        "print(f\"Dtype: {X.dtype}\")\n",
        "#come si fa a trasformarli in scala di grigi ?"
      ],
      "metadata": {
        "id": "TX7ilYaHyqHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_gray = np.mean( X, axis=-1)\n",
        "print(f\"Img shape{x_gray[0].shape}\")\n",
        "print(f\"Min: {np.min(x_gray)}\")\n",
        "print(f\"Max: {np.max(x_gray)}\")\n",
        "print(f\"Dtype: {x_gray.dtype}\")\n",
        "plt.imshow(x_gray[0], cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "--y5-RRdz3JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data preprocessing\n",
        "\n",
        "\n",
        "1.   Normalizzazione\n",
        "2.   Sbilanciamento delle classi\n",
        "3.   Train Test split stratificato"
      ],
      "metadata": {
        "id": "2l3PAhQqfkoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Datasets Normalization**"
      ],
      "metadata": {
        "id": "ulb7Pit9iiy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_samples = 1000\n",
        "n_features = 2\n",
        "n_classes = 2\n",
        "random_seed = 42\n",
        "noise_factor = 0.1\n",
        "\n",
        "# Genera il dataset con make_blobs\n",
        "X, y = make_blobs(n_samples=n_samples, n_features=n_features, centers=n_classes, random_state=random_seed)\n",
        "\n",
        "plt.scatter( X[:,0], X[:,1], c=y, cmap=\"RdGy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gwX17p1UpnK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abbiamo una distribuzione e vogliamo Normalizzarla per dare a tutte le features la stessa scala."
      ],
      "metadata": {
        "id": "DpUTXI0Wqw2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset preprocessing\n",
        "from sklearn import preprocessing\n",
        "\n",
        "zScoreScaler = preprocessing.StandardScaler() # z = (x - u) / s\n",
        "minMaxScaler = preprocessing.MinMaxScaler() # z = (x-x_min)/(x_max-x_min) : range [0,1]\n",
        "maxAbsScaler = preprocessing.MaxAbsScaler() # z = x / Abs(x_max)\n",
        "\n",
        "scalers = [zScoreScaler, minMaxScaler,maxAbsScaler]\n",
        "titles = [\"Z score normalization\", \"MinMax normalization\",\"Max Absolute Normalization\"]\n",
        "\n",
        "num_cols = len(scalers)\n",
        "fig_width = 6  # Adjust this value as needed\n",
        "fig_height = fig_width * num_cols\n",
        "fig,axs = plt.subplots( 1,num_cols,figsize=(fig_height,fig_width ))\n",
        "\n",
        "for i, (title,scaler) in enumerate(zip(titles,scalers)):\n",
        "  #General api interface\n",
        "  scaler.fit(X)\n",
        "  X_scaled = scaler.transform(X)\n",
        "  axs[i].scatter( X_scaled[:,0], X_scaled[:,1], c=y, cmap=\"RdGy\")\n",
        "  axs[i].set_title(title)\n",
        "\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cfFwXvx5q4YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un altro scaler importante è log normalization."
      ],
      "metadata": {
        "id": "nxbuZ6wozScL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stratification**"
      ],
      "metadata": {
        "id": "YIVD1uVre8x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stampa_percentuale( y, title=\"\" ):\n",
        "  if title:print(title)\n",
        "  # Calcoliamo le occorrenze di ciascuna classe nel dataset\n",
        "  unique, counts = np.unique(y, return_counts=True)\n",
        "  percentuali = (counts / len(y)) * 100 # calcolo la % di occorrenze per ciascuna classe\n",
        "  # Stampiamo le occorrenze e le percentuali\n",
        "  for classe, conteggio, percentuale in zip(unique, counts, percentuali):\n",
        "      print(f\"Classe {classe}: Occorrenze = {conteggio}, Percentuale {percentuale} %\" )\n",
        "  print(f\"Totale occorrenze : {sum(counts)}\")\n",
        "  print()\n",
        "\n",
        "def genera_dataset(n_samples=1000, w=[0.5, 0.5], seed=42):\n",
        "    if sum(w) != 1.0:\n",
        "      raise ValueError(\"La somma dei pesi in 'w' deve essere uguale a 1.0\")\n",
        "\n",
        "    n_classes = len(w)\n",
        "    X = np.random.randint(0, 1500, n_samples)\n",
        "    occorrenze = [int(n_samples * peso) for peso in w ]\n",
        "    y = np.concatenate([np.full(occorrenza, classe) for classe, occorrenza in enumerate(occorrenze)])\n",
        "    return X, y\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X,y = genera_dataset(w=[0.8,0.2] )\n",
        "\n",
        "stampa_percentuale( y, \"Dataset completo\" )\n",
        "\n",
        "# Esegui uno splitting stratificato\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42) #stratify è importante\n",
        "\n",
        "stampa_percentuale( y_train, \"Training set splittato\" )\n",
        "stampa_percentuale( y_test, \"Test set splittato\" )\n",
        "\n"
      ],
      "metadata": {
        "id": "3PKXKvg7DbDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Lo splitting stratificato è una pratica raccomandata quando si lavora con dataset di machine learning, specialmente quando ci sono classi sbilanciate. La sua importanza risiede nel fatto che assicura che i dati di test riflettano accuratamente la distribuzione dei dati reali, contribuendo così a mantenere la rappresentatività dei dati di addestramento anche nel test set. Questo migliora la generalizzazione del modello, previene il bias nell'apprendimento e consente di valutare in modo equo le prestazioni del modello.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NU7XGGWd3CtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Balancing ed Unbalancing**"
      ],
      "metadata": {
        "id": "XM4egG0D7qCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "pip install imbalanced-learn"
      ],
      "metadata": {
        "id": "30E9cfS973xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.under_sampling import RandomUnderSampler,NearMiss\n",
        "\n",
        "\n",
        "\n",
        "# Genera un dataset sbilanciato di esempio\n",
        "X, y = make_classification(\n",
        "    n_classes=2,\n",
        "    class_sep=2,\n",
        "    weights=[0.1, 0.9],\n",
        "    n_informative=3,\n",
        "    n_redundant=1,\n",
        "    flip_y=0,\n",
        "    n_features=5,\n",
        "    n_clusters_per_class=1,\n",
        "    n_samples=1000,\n",
        "    random_state=42)\n",
        "\n",
        "# Applica Il metodo per l'undersampling\n",
        "undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "# undersampler = NearMiss(sampling_strategy='auto', version=2, n_neighbors=3)\n",
        "\n",
        "X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
        "\n",
        "# Plotta il dataset originale e il dataset resampled\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='Set1', marker='o')\n",
        "plt.title('Dataset originale')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=y_resampled, cmap='Set1', marker='o')\n",
        "plt.title('Dataset Ricampionato')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "stampa_percentuale( y, \"Dataset originale\" )\n",
        "stampa_percentuale( y_resampled, \"Dataset ricampionato\" )\n"
      ],
      "metadata": {
        "id": "dMIihu_58QlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Oversampling**"
      ],
      "metadata": {
        "id": "pUxxbBqF94Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.over_sampling import ADASYN,SMOTE\n",
        "\n",
        "# Genera un dataset sbilanciato di esempio\n",
        "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=5, n_clusters_per_class=1, n_samples=1000, random_state=42)\n",
        "\n",
        "# Applica SMOTE per l'oversampling\n",
        "oversampler_adasyn = ADASYN(sampling_strategy='auto', random_state=42, n_neighbors=6)\n",
        "oversampler_smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "\n",
        "X_adasyn_resampled, y_adasyn_resampled = oversampler_adasyn.fit_resample(X, y)\n",
        "X_smote_resampled, y_smote_resampled = oversampler_smote.fit_resample(X, y)\n",
        "\n",
        "\n",
        "# Plotta il dataset originale e il dataset resampled\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='Set1', marker='o')\n",
        "plt.title('Dataset originale')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(X_adasyn_resampled[:, 0], X_adasyn_resampled[:, 1], c=y_adasyn_resampled, cmap='Set1', marker='o')\n",
        "plt.title('Dataset Ricampionato con ADASYN')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(X_smote_resampled[:, 0], X_smote_resampled[:, 1], c=y_smote_resampled, cmap='Set1', marker='o')\n",
        "plt.title('Dataset Ricampionato con SMOTE')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "stampa_percentuale( y, \"Dataset originale\" )\n",
        "stampa_percentuale( y_adasyn_resampled, \"Dataset ricampionato ADASYN\" )\n",
        "stampa_percentuale( y_smote_resampled, \"Dataset ricampionato SMOTE\" )\n"
      ],
      "metadata": {
        "id": "SCunmI-K94AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task di Classificazione"
      ],
      "metadata": {
        "id": "_jJDLvHSn9yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classificazione Binaria"
      ],
      "metadata": {
        "id": "4yLhEoy3iw2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esempi di tasks di classificazione binaria"
      ],
      "metadata": {
        "id": "_eAT3YQOjKrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "rFLg2TjZMhY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#importiamo 3 modelli da sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "#dataset generation\n",
        "\n",
        "from sklearn.datasets import make_blobs,make_moons,make_s_curve,make_circles\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_samples = 1000\n",
        "n_features = 2\n",
        "n_classes = 2\n",
        "seed = 42\n",
        "noise_factor = 0.1\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Genera il dataset con make_blobs\n",
        "X, y = make_blobs(n_samples=n_samples, n_features=n_features, centers=n_classes, random_state=seed,  )#cluster_std=[1.,2.]\n",
        "# X, y = make_moons(n_samples=n_samples, noise=noise_factor, random_state=seed)\n",
        "# X, y = make_circles(n_samples=n_samples, noise=noise_factor, factor=0.1, random_state=seed)\n",
        "\n",
        "\n",
        "## Data preprocessing\n",
        "# Creiamo un dataset di esempio con due classi in proporzione diversa\n",
        "scaler = preprocessing.MinMaxScaler() # range [0,1]\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "\n",
        "\n",
        "#mostro la distribuzione del dataset a video\n",
        "plt.scatter( X[:,0], X[:,1], c=y, cmap=\"viridis\")\n",
        "plt.show()\n",
        "\n",
        "# Esegui uno splitting stratificato\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed) #stratify è importante\n",
        "\n",
        "# Crea i modelli\n",
        "# Creazione del modello di regressione logistica\n",
        "model_logistic = LogisticRegression(\n",
        "    penalty='l2',     # Tipo di regolarizzazione (l1 o l2)\n",
        "    C=1.0,            # Inverso della forza di regolarizzazione\n",
        "    random_state=42   # Seed per la riproducibilità\n",
        ")\n",
        "\n",
        "\n",
        "# Creazione del modello GaussianNB\n",
        "model_naive_bayes = GaussianNB()\n",
        "\n",
        "\n",
        "# Creazione del modello dell'albero decisionale con parametri personalizzati\n",
        "model_decision_tree = DecisionTreeClassifier(\n",
        "    criterion='entropy',  # Criterio per la suddivisione ('gini' o 'entropy')\n",
        "    max_depth=None,       # Profondità massima dell'albero\n",
        "    min_samples_split=2,  # Numero minimo di campioni richiesti per suddividere un nodo\n",
        "    min_samples_leaf=1,   # Numero minimo di campioni richiesti in una foglia\n",
        "    max_features=None,    # Numero di caratteristiche da considerare per la divisione\n",
        "    random_state=seed     # Seed per la riproducibilità\n",
        ")\n",
        "\n",
        "\n",
        "# Addestramento dei modelli\n",
        "\n",
        "model_logistic.fit(X,y)\n",
        "model_naive_bayes.fit(X,y)\n",
        "model_decision_tree.fit(X,y)"
      ],
      "metadata": {
        "id": "wTKzYAWXBaiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vediamo cosa hanno imparato\n",
        "\n",
        "# Predici le classi per i dati di test\n",
        "y_pred_logistic = model_logistic.predict(X_test)\n",
        "y_pred_naive_bayes = model_naive_bayes.predict(X_test)\n",
        "y_pred_decision_tree = model_decision_tree.predict(X_test)\n",
        "\n",
        "# # Crea un meshgrid per visualizzare la decision boundary\n",
        "x_min = X_test[:, 0].min() - 0.1\n",
        "x_max = X_test[:, 0].max() + 0.1\n",
        "\n",
        "xv = np.linspace(x_min,x_max, 500) # Questi valori rappresentano 500 punti lungo l'asse delle ascisse (X).\n",
        "yv = np.linspace(x_min,x_max, 500) # Questi valori rappresentano 500 punti lungo l'asse delle ordinate (Y).\n",
        "\n",
        "xx, yy = np.meshgrid(yv,xv) # creare un meshgrid bidimensionale.\n",
        "# xx contiene i punti X della matrice XY lungo l'asse X\n",
        "# yy conterrà i punti Y della matrice XY lungo l'asse Y\n",
        "\n",
        "\n",
        "# Visualizza le decision boundary dei tre modelli\n",
        "models = {\n",
        "    'Regressione Logistica':model_logistic,\n",
        "    'Naive Bayes': model_naive_bayes,\n",
        "    'Albero di Decisione':model_decision_tree\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "for i, (model_name, model) in enumerate(models.items(), 1):\n",
        "    plt.subplot(1, 3, i)\n",
        "\n",
        "    x_vec = xx.ravel() #array delle X della mesh\n",
        "    y_vec = yy.ravel() #array delle Y della mesh\n",
        "\n",
        "    zv = model.predict(np.c_[xx.ravel(), yy.ravel()])  # array delle Z (colore) della mesh\n",
        "    zz = zv.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, zz, alpha=0.4)\n",
        "    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, marker='o', edgecolor='k', cmap=plt.cm.Set1)\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title(model_name)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "skAmyL-ZSTmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classificazione multi-classe"
      ],
      "metadata": {
        "id": "B3r8bj-Ri0k9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esempio di tasks di classificazione multi-classe"
      ],
      "metadata": {
        "id": "Ht6pWQDYjOOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "seed=42\n",
        "iris_dataset_pd = load_iris(as_frame=True) #as_frame torna un oggetto che contiene\n",
        "X = iris_dataset_pd.data   #è un DataFrame pandas\n",
        "y= iris_dataset_pd.target  #è un Series pandas\n",
        "classes_dict = ['Setosa','Versicolor','Virginica']\n",
        "\n",
        "print(f\"Input features are: { X.columns.tolist() }, shape { X.shape }\")\n",
        "print(f\"Output features are: { y.unique().tolist()}\")\n",
        "print(f\"Output label are: { classes_dict }\")\n",
        "\n",
        "#Data preprocessing\n",
        "scaler = preprocessing.MinMaxScaler() # range [0,1]\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "\n",
        "#Data splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed) #stratify è importante\n",
        "\n",
        "#Training\n",
        "# Crea i modelli\n",
        "# Creazione del modello di regressione logistica\n",
        "model_logistic = LogisticRegression(\n",
        "    penalty='l2',     # Tipo di regolarizzazione (l1 o l2)\n",
        "    C=1.0,            # Inverso della forza di regolarizzazione\n",
        "    random_state=42   # Seed per la riproducibilità\n",
        ")\n",
        "\n",
        "\n",
        "# Creazione del modello GaussianNB\n",
        "model_naive_bayes = GaussianNB()\n",
        "\n",
        "\n",
        "# Creazione del modello dell'albero decisionale con parametri personalizzati\n",
        "model_decision_tree = DecisionTreeClassifier(\n",
        "    criterion='entropy',  # Criterio per la suddivisione ('gini' o 'entropy')\n",
        "    max_depth=None,       # Profondità massima dell'albero\n",
        "    min_samples_split=2,  # Numero minimo di campioni richiesti per suddividere un nodo\n",
        "    min_samples_leaf=1,   # Numero minimo di campioni richiesti in una foglia\n",
        "    max_features=None,    # Numero di caratteristiche da considerare per la divisione\n",
        "    random_state=seed     # Seed per la riproducibilità\n",
        ")\n",
        "\n",
        "\n",
        "# Addestramento dei modelli\n",
        "\n",
        "model_logistic.fit(X_train,y_train)\n",
        "model_naive_bayes.fit(X_train,y_train)\n",
        "model_decision_tree.fit(X_train,y_train)\n",
        "\n",
        "#preprocessing che non abbiamo visto prima ( sul testo, data cleaning, sulle immagini di iris ad esempio )\n",
        "\n",
        "#spiego la cross-validation come si usa e perchè è importante per valutare i modelli\n",
        "\n",
        "#mostro le metriche ed i loro calcoli per concludere"
      ],
      "metadata": {
        "id": "mpjrsUzUtz3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Valutiamo i modelli\n",
        "![image](https://drive.google.com/uc?id=1pLgFVYNR6zGdNNN6yBCqahniP0HdxuW7 )"
      ],
      "metadata": {
        "id": "zjhM8cTXPVYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Definisci i modelli\n",
        "modelli = [\n",
        "    (\"Regressione Logistica\", model_logistic),\n",
        "    (\"Naive Bayes\", model_naive_bayes),\n",
        "    (\"Albero Decisionale\", model_decision_tree)\n",
        "]\n",
        "\n",
        "# Imposta il numero di fold\n",
        "k = 5\n",
        "\n",
        "# Crea oggetti StratifiedKFold per la cross-validation\n",
        "cv = StratifiedKFold(n_splits=k)\n",
        "\n",
        "# Crea una figura per i subplot\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Itera attraverso i modelli\n",
        "for i, (nome_modello, modello) in enumerate(modelli, 1):\n",
        "\n",
        "    # Calcola i risultati della cross-validation per il modello corrente\n",
        "    results = cross_val_score(modello, X_train, y_train, cv=cv)\n",
        "\n",
        "    # Crea un subplot per il modello corrente\n",
        "    plt.subplot(1, len(modelli), i)\n",
        "\n",
        "    # Plotta i risultati\n",
        "    plt.bar(range(1, k+1), results)\n",
        "    plt.xlabel('Fold')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(f'{nome_modello} - K-Fold Cross-Validation')\n",
        "    plt.ylim(0.8, 1.0)\n",
        "\n",
        "# Aggiungi titolo alla figura\n",
        "plt.suptitle('Risultati K-Fold Cross-Validation per Diversi Modelli', y=1.02)\n",
        "\n",
        "# Mostra i subplots\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "qtn-2Y9X2-jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "se ci sono differenze significative fra le accuracy dei vari fold vuol dire che il modello ha prestazioni variabili su diversi fold o che alcuni fold sono più difficili da predire rispetto ad altri. Quindi potremmo provare a fare meglio. ad esempio con un ensemble."
      ],
      "metadata": {
        "id": "inpw32qW5nKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Crea l'ensemble dei modelli con il metodo di voto\n",
        "ensemble_model = VotingClassifier(\n",
        "    estimators=[\n",
        "        (\"Regressione Logistica\", model_logistic),\n",
        "        (\"Naive Bayes\", model_naive_bayes),\n",
        "        (\"Albero Decisionale\", model_decision_tree)\n",
        "    ],\n",
        "    voting='soft'  # 'soft' per il voto basato sulle probabilità\n",
        ")\n",
        "ensemble_model.fit(X_train,y_train)\n",
        "\n",
        "# Esegui la cross-validation sul modello ensemble\n",
        "results_ensemble = cross_val_score(ensemble_model, X_train, y_train, cv=cv)\n",
        "\n",
        "# Plotta i risultati del modello ensemble\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(range(1, k+1), results_ensemble)\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Ensemble - K-Fold Cross-Validation')\n",
        "plt.ylim(0.8, 1.0)  # Imposta il limite dell'asse y per una migliore visualizzazione\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j6FxVj1_3ieK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#valutiamo chi ha fatto meglio con la marice di confusione e tutte le sue metriche\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def confronta_modelli(X_test, y_test, modelli):\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(modelli), figsize=(15, 5))  # Crea una riga di grafici\n",
        "    results = []\n",
        "    for i, (nome_modello, modello) in enumerate(modelli):\n",
        "      y_pred = modello.predict(X_test)\n",
        "\n",
        "      accuracy = accuracy_score(y_test, y_pred)\n",
        "      precision = precision_score(y_test, y_pred, average=\"macro\")\n",
        "      recall = recall_score(y_test, y_pred, average=\"macro\")\n",
        "      f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
        "      conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "      # Plotta la matrice di confusione\n",
        "      ConfusionMatrixDisplay(conf_matrix, display_labels=['Setosa','Versicolor','Virginica']).plot(ax=axes[i] )\n",
        "      axes[i].set_title(f\"Matrice di Confusione - {nome_modello}\")\n",
        "\n",
        "      risultato_modello = {\n",
        "            \"Modello\": nome_modello,\n",
        "            \"Accuratezza\": accuracy,\n",
        "            \"Precisione\": precision,\n",
        "            \"Recall\": recall,\n",
        "            \"F1-Score\": f1\n",
        "      }\n",
        "\n",
        "      results.append(risultato_modello)\n",
        "\n",
        "    results_df = pd.DataFrame(columns=[\"Modello\", \"Accuratezza\", \"Precisione\", \"Recall\", \"F1-Score\"], data=results)\n",
        "\n",
        "    print(results_df.to_markdown(index=False))\n",
        "    # Mostra i subplot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "modelli = [\n",
        "    (\"LR\", model_logistic),\n",
        "    (\"NB\", model_naive_bayes),\n",
        "    (\"DT\", model_decision_tree),\n",
        "    (\"Ens\", ensemble_model)\n",
        "]\n",
        "\n",
        "# Ottieni i risultati delle metriche per tutti i modelli\n",
        "confronta_modelli(X_test, y_test, modelli)"
      ],
      "metadata": {
        "id": "8blDnSlg3Xmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy** (Accuratezza): L'accuracy è una misura della percentuale di previsioni corrette fatte dal modello rispetto al numero totale di previsioni.\n",
        "\n",
        "$$\n",
        "Accuracy = \\frac{Numero\\ di\\ previsioni\\ corrette}{Numero\\ totale\\ di\\ previsioni}\n",
        "$$\n",
        "\n",
        "**Precision** (Precisione):Si concentra sulla percentuale di previsioni positive fatte dal modello che erano effettivamente corrette.\n",
        "\n",
        "$$\n",
        "Precision = \\frac{True\\ Positives}{True\\ Positives + False\\ Positives}\n",
        "$$\n",
        "\n",
        "\n",
        "**Recall** (Recall o True Positive Rate): Il recall è una misura della frazione di esempi positivi reali che sono stati correttamente previsti dal modello.\n",
        "\n",
        "$$\n",
        "Recall = \\frac{True\\ Positives}{True\\ Positives + False\\ Negatives}\n",
        "$$\n",
        "\n",
        "\n",
        "**F1-Score** (F1-Score o F1 Measure): Il F1-Score è una metrica che combina precisione e recall in un'unica misura. È il valore medio armonico tra precisione e recall.\n",
        "\n",
        "$$\n",
        "F1-Score = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TOlMPcw6-zpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UtJ0jhMA_ygl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q7bDysmajWdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dYfHUrjhDOpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u3aeaDlNqVhL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}