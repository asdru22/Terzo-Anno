{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lezione 3 - Unsupervised Tasks\n",
        "- Clustering\n",
        "- PCA\n",
        "- Anomaly Detections"
      ],
      "metadata": {
        "id": "w9MrdCbDuse4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clustering"
      ],
      "metadata": {
        "id": "U4tooA1qvB1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il clustering è una metodo di analisi dati utilizzato nell'apprendimento automatico e nella statistica per raggruppare dati simili in gruppi in cui gli oggetti all'interno dello stesso cluster risultano più simili tra di loro e meno simili rispetto a quelli appartenenti a cluster diversi.\n",
        "\n",
        "Analizzando le relazioni o le similitudini tra i dati all'interno dello stesso cluster, inoltre, si possono scoprire informazioni non immediatamente evidenti quando si osserva l'insieme completo dei dati.\n",
        "\n",
        "\n",
        "Esempi applicativi dove è possibile scoprire strutture dati ad esempio sono:\n",
        "\n",
        "- Costumer Segmentation.\n",
        "- Ricerca di Mercato.\n",
        "- Analisi delle recensioni online.\n",
        "- Raggruppare documenti o immagini direttamente dal loro contenuto."
      ],
      "metadata": {
        "id": "DIC3RL3OvFzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Algoritmi di Clustering\n",
        "\n",
        "Ogni approccio di clustering può funzionare meglio per una particolare distribuzione di dati.\n",
        "\n",
        "Vedremo 4 \"famiglie\" di algoritmi. In questo lavoro è presente una lista esaustiva: https://link.springer.com/article/10.1007/s40745-015-0040-1\n"
      ],
      "metadata": {
        "id": "zSwxW4__Eojj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Centroid-based Clustering**\n",
        "Suddivide i dati in cluster in modo che ciascun cluster sia rappresentato da un \"centroide,\" che è il punto medio o il centro geometrico di tutti gli oggetti nel cluster.Gli oggetti vengono assegnati al cluster con il centroide più vicino.  **KMeans** è l'esempio tipico per questo tipo di algoritmo.\n"
      ],
      "metadata": {
        "id": "c6AVGHYIEuLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://drive.google.com/uc?id=1vQbJ5iotcJb5IjnM57eqDyqLJ-8v9UBn)\n"
      ],
      "metadata": {
        "id": "fMApaPIcExFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Density-based**\n",
        "Quando un'area di un piano contiene molti punti dei dati vicini tra loro, ciò indica un'alta densità in quella regione. Questo suggerisce la possibile presenza di un cluster, poiché gli oggetti simili sono strettamente raggruppati.\n",
        "\n",
        "Il clustering basato sulla densità collega le regioni di elevata densità degli esempi in cluster. Ciò consente distribuzioni di forma arbitraria, a patto che le aree dense possano essere collegate. Questi algoritmi hanno difficoltà con dati di densità variabile e in dimensioni elevate. Inoltre, per progettazione, questi algoritmi non assegnano gli outlier ai cluster.\n",
        "Un esempio di tecnica di clustering basata sulla densità è l'algoritmo **DBSCAN**."
      ],
      "metadata": {
        "id": "uhc6eUfwFBjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![image](https://drive.google.com/uc?id=1niBupjn2dbiRiHxv9x2RVkVSZYIdy3EI)\n"
      ],
      "metadata": {
        "id": "54swzof8E0EJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### **Distribution-based Clustering**\n",
        "\n",
        "Questo approccio di clustering assume che i dati seguano una certa distribuzione, ad esempio la Gaussiana. L'algoritmo di clustering basato sulla distribuzione assegna gli oggetti a cluster in modo che seguano una distribuzione specifica.  Un esempio di questo tipo di clustering è **il Gaussian Mixture Model (GMM)** clustering, che assume che i dati seguano una distribuzione gaussiana."
      ],
      "metadata": {
        "id": "h4R_vtxxE9f7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://drive.google.com/uc?id=14UjNnNzR39KbdZ_FTgiB7aW-EXapRi6_ )"
      ],
      "metadata": {
        "id": "Wy2Ww0duFL2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Se abbiamo delle distribuzioni gaussiane, all'aumentare della distanza dal centro della distribuzione, diminuisce la probabilità che un punto appartenga a quella distribuzione. Tuttavia, quando non si conosce il tipo di distribuzione nei dati, è opportuno utilizzare un algoritmo diverso.\n",
        "In figura le bande mostrano tale diminuzione di probabilità."
      ],
      "metadata": {
        "id": "hVPlD6njFRLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### **Hierarchical Clustering**\n",
        "\n",
        "L'approccio hierarchical clustering coinvolge la creazione di una gerarchia di cluster, in cui i cluster più piccoli sono combinati progressivamente per formare cluster più grandi. Questo processo può essere rappresentato graficamente come un dendrogramma, che mostra la struttura gerarchica dei cluster. Esistono diversi metodi per l'agglomerazione (combinazione) e la divisione dei cluster in questo approccio. Sklearn ad esempio fornisce **AgglomerativeClustering**\n",
        "\n"
      ],
      "metadata": {
        "id": "zn-ZuzDb13dQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![image](https://drive.google.com/uc?id=1tcXBSzV5pLHlUq6w8b_66XiQbZmWRVzN )"
      ],
      "metadata": {
        "id": "NFMbDj7aFUpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Considerazioni finali\n",
        "\n",
        "\n",
        "Questi quattro approcci forniscono diverse metodologie per suddividere i dati in cluster in base a criteri diversi, a seconda delle caratteristiche dei dati e degli obiettivi dell'analisi.\n",
        "\n",
        "Quando si sceglie quale algoritmo da utilizzare è importante anche considerare se l'algoritmo scala nel dataset. In Machine Learning ci sono dataset con milioni di esempi ma non tutti gli algoritmi di clustering scalano efficientemente. Alcuni di loro calcolano la similarità fra tutte le coppie di esempii e questo vuol dire che a runtime il numero di esempi sale in o(n^2).\n",
        "Tali algoritmi nella pratica quando si hanno milioni di esempi sono computazionalmente troppo costosi.\n",
        "\n",
        "Ad esempio:\n",
        "**Kmeans** scala linearmente e ha una complessità di O(n * k * i * d). Dove k sono i centroidi, i le iterazioni, d la dimensione dei dati.\n",
        "\n",
        "**DBASCN** invece ha una complessità di O(n * log(n)) ma in alcuni casi se i dati sono molto densi diventa  O(n^2), dove \"n\" rappresenta il numero di punti dati.\n"
      ],
      "metadata": {
        "id": "EPmbwsue3DoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prendiamo i 3 dataset della volta scorsa ed effettuiamo il clustering dei punti.\n",
        "\n",
        "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "#importiamo 3 modelli da sklearn\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "#dataset generation\n",
        "from sklearn.datasets import make_blobs,make_moons,make_s_curve,make_circles\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "n_samples = 1000\n",
        "n_features = 2\n",
        "n_classes = 2\n",
        "seed = 42\n",
        "noise_factor = 0.1\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Genera il dataset con make_blobs\n",
        "# X, y = make_blobs(n_samples=n_samples, n_features=n_features, centers=n_classes, random_state=seed)\n",
        "# X, y = make_blobs(n_samples=n_samples, n_features=n_features, centers=n_classes, random_state=seed , cluster_std=[1.,2.] )\n",
        "# X, y = make_moons(n_samples=n_samples, noise=noise_factor, random_state=seed, )\n",
        "X, y = make_circles(n_samples=n_samples, noise=noise_factor, factor=0.1, random_state=seed)\n",
        "\n",
        "\n",
        "## Data preprocessing\n",
        "# Creiamo un dataset di esempio con due classi in proporzione diversa\n",
        "scaler = preprocessing.MinMaxScaler() # range [0,1]\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "\n",
        "\n",
        "#mostro la distribuzione del dataset a video\n",
        "plt.scatter( X[:,0], X[:,1], c=y, cmap=\"viridis\")\n",
        "plt.show()\n",
        "\n",
        "# Esegui uno splitting stratificato\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed) #stratify è importante\n",
        "\n",
        "\n",
        "#DBSCAN e agglomerative clustering hanno un approccio \"lazy\",\n",
        "#ovvero non richiedono un training ma sono algoritmi che possiamo applicare direttamente ai dati di test.\n",
        "\n",
        "def plot_clusters(x_axis,y_axis, y_predict, title):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(x_axis, y_axis, c=y_predict, cmap='viridis')\n",
        "    plt.title(title)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "f-LTFVLZJ74Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creazione e addestramento di K-Means\n",
        "kmeans = KMeans(n_clusters=2, random_state=seed)\n",
        "kmeans.fit(X_train)\n",
        "\n",
        "kmeans_labels = kmeans.predict(X_test)\n",
        "\n",
        "\n",
        "# Creazione e addestramento di GMM\n",
        "gmm = GaussianMixture(n_components=2, covariance_type='full', random_state=seed)\n",
        "gmm.fit(X_train)\n",
        "gmm_labels = gmm.predict(X_test)\n",
        "\n",
        "# Creazione di DBSCAN\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=2 ) # i dati sono normalizzati in [0,1] ed eps è il raggio che serve nell'intersezione\n",
        "dbscan_labels = dbscan.fit_predict(X_test)\n",
        "\n",
        "# Creazione di AgglomerativeClustering\n",
        "#il metodo \"Ward\" cerca di creare cluster che minimizzano la varianza.\n",
        "#in questo modo si tengono i cluster omogenei e compatti.\n",
        "ward = AgglomerativeClustering(n_clusters=2,linkage=\"ward\" )#\n",
        "ward=ward.fit(X_train)\n",
        "ward_labels = ward.fit_predict(X_test)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16, 4))\n",
        "# Plot K-Means\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=kmeans_labels, cmap='viridis')\n",
        "plt.title('K-Means')\n",
        "\n",
        "# Plot GMM\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=gmm_labels, cmap='viridis')\n",
        "plt.title('GMM')\n",
        "\n",
        "# Plot DBSCAN\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=dbscan_labels, cmap='viridis')\n",
        "plt.title('DBSCAN')\n",
        "\n",
        "\n",
        "# Plot Agglomerative Clustering con metodo \"Ward\"\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=ward_labels, cmap='viridis')\n",
        "plt.title('WARD')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "lcoj73p1imhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KMEANS, GMM** e **Hierarchical** clustering, per loro definizione non possono funzionare in questo caso. Serve apportare modifiche alla distribuzione."
      ],
      "metadata": {
        "id": "xOucs_JqbrWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dimensionality Reduction\n",
        "\n",
        "In campo di apprendimento automatico, la Principal Component Analysis (PCA) è una tecnica popolare per affrontare il problema della riduzione della dimensionalità.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eJtQbwowFjqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PCA\n",
        "\n",
        "PCA trasforma un dataset ad alta dimensionalità in uno spazio a dimensioni inferiori, semplificandone la visualizzazione e accelerando altre analisi dei dati. Ma PCA non riduce semplicemente la dimensionalità dei dati: mira a catturare le strutture più significative nei dati, preservando il più possibile la variabilità.\n",
        "\n",
        "Questo viene realizzato identificando le componenti principali in cui i dati variano di più e proiettando i dati su queste direzioni. Le componenti principali stesse sono combinazioni lineari delle caratteristiche originali, ortogonali tra loro, garantendo che non ci siano informazioni ridondanti. La prima componente principale cattura la varianza più alta nei dati, la seconda componente principale (ortogonale alla prima) cattura la seconda varianza più alta e così via. Rappresentando i dati in termini di queste componenti, PCA ci fornisce un modo per esprimere i dati in un numero ridotto di dimensioni che conserva le strutture essenziali.\n",
        "\n",
        "Per fare un esempio, immagina un insieme di punti dati nello spazio tridimensionale che si trova principalmente su una superficie piatta. Nonostante i dati siano tridimensionali, la loro struttura intrinseca può essere catturata utilizzando solo due dimensioni: la superficie. PCA identifica questa superficie e ci consente di rappresentare ciascun punto dati tramite le sue coordinate sulla superficie anziché nello spazio tridimensionale completo.\n"
      ],
      "metadata": {
        "id": "tLx-a_zQpE-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA in pratica"
      ],
      "metadata": {
        "id": "1gN844Y0pdPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prendiamo il dataset di iris che avevamo visto.\n",
        "Aveva 4 features di input e ci veniva male mostrarle a video tutte insieme.\n",
        "Vogliamo ridurle per avere una visione migliore di queste features e scoprire eventuali patterns.\n"
      ],
      "metadata": {
        "id": "ofdin04F1Qe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Carica il dataset Iris\n",
        "iris = load_iris(as_frame=True)\n",
        "X = iris.data  # Carica i dati Iris\n",
        "\n",
        "# Standardizzazione dei dati (mu 0, std 1)\n",
        "X_std = StandardScaler().fit_transform(X)\n",
        "\n",
        "print(f\"Input shape: {X_std.shape}\")\n",
        "print(f\"Numero di esempi: {X_std.shape[0]}\")\n",
        "print(f\"Numero di features (Input Dimensionality): {X_std.shape[1]}\")"
      ],
      "metadata": {
        "id": "Ieftl4MB3dFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Usiamo la pca per ridurre le features\n",
        "\n",
        "n_comp = 2\n",
        "pca = PCA(n_components=n_comp)\n",
        "\n",
        "# Addestramento della PCA sui dati standardizzati\n",
        "X_reduced = pca.fit_transform(X_std)\n",
        "\n",
        "print(f\"Dataset ridotto shape: {X_reduced.shape}\")\n",
        "print(f\"Numero di esempi: {X_reduced.shape[0]}\")\n",
        "print(f\"Numero di features (Input Dimensionality): {X_reduced.shape[1]}\")\n",
        "\n",
        "\n",
        "# Crea una figura 2D o 3D\n",
        "fig = plt.figure()\n",
        "\n",
        "if n_comp==3:\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "  ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=iris.target, cmap='viridis')\n",
        "  ax.view_init(elev=10, azim=90)  # Angolo di vista in gradi ed elevazione\n",
        "  ax.set_zlabel('Terza componente principale')\n",
        "else:\n",
        "  ax = fig.add_subplot(111)\n",
        "  ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=iris.target, cmap='viridis')\n",
        "\n",
        "\n",
        "# Aggiungi etichette per gli assi\n",
        "ax.set_xlabel('Prima componente principale')\n",
        "ax.set_ylabel('Seconda componente principale')\n",
        "ax.set_title('PCA del dataset Iris')\n",
        "# Mostra il grafico\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cBWsz8BHpXNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Come si legge la PCA ?\n",
        "\n",
        "\n",
        "\n",
        "1.   Varianza spiegata da ciascun componente:\n",
        "  Se ho 4 features, e li riduco a 2 la varianza spiegata è un grafo a 2 barre dove ogni barra indica quanto una componente riesce a rappresentare gli esempi rappresentati dalle 4 features.\n",
        "  Es nel nostro caso la prima componente cattura il 70% della varianza totale dei dati, la seconda cattura il 30%.\n",
        "  La somma delle varianze spiegate da tutte le componenti principali sarà uguale a 1.\n",
        "\n",
        "2.   Varianza spiegata cumulativa:\n",
        "  Se ordiniamo le componenti e li sommiamo, ad ogni somma abbiamo la varianza spiegata dalle prime n componenti rispetto alla varianza totale.\n",
        "  E quindi ti fornisce una visione collettiva.\n",
        "\n",
        "\n",
        "Sono metriche utili per decidere il numero di componenti.\n"
      ],
      "metadata": {
        "id": "Tte-hQAk-sPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Varianza spiegata\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "explained_variance_cumulative = np.cumsum(explained_variance)\n",
        "\n",
        "# Plot della varianza spiegata\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(range(1, len(explained_variance) + 1), explained_variance)\n",
        "plt.xticks(range(1, len(explained_variance) + 1))\n",
        "\n",
        "plt.xlabel('Componenti Principali')\n",
        "plt.ylabel('Varianza Spiegata')\n",
        "plt.title('Varianza Spiegata da ciascuna componente principale')\n",
        "\n",
        "# Plot della varianza spiegata cumulativa\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(explained_variance_cumulative) + 1), explained_variance_cumulative, marker='o', linestyle='--', color='r')\n",
        "plt.xticks(range(1, len(explained_variance) + 1))\n",
        "plt.xlabel('Componenti Principali')\n",
        "plt.ylabel('Varianza Spiegata Cumulativa')\n",
        "plt.title('Varianza Spiegata Cumulativa')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CvRSkdZz90lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nella riduzione di dimensionalità, per le tecniche quali PCA o altre ad esempio AutoEncoders, alcune delle informazioni originali vengono eliminate.Le componenti principali estratte rappresentano una sintesi delle informazioni contenute nelle features originali. Esiste un Trade-Off tra dimensione e informazione.\n"
      ],
      "metadata": {
        "id": "sKC_GDnt1Eta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Approfondimento PCA"
      ],
      "metadata": {
        "id": "XLLxM4FsjaWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "y_std = y\n",
        "# Standardize the data\n",
        "X_std = StandardScaler().fit_transform(X)\n",
        "\n",
        "#Calcola la matrice delle covarianze\n",
        "covariance_matrix = np.cov(X_std.T)\n",
        "\n",
        "#computa EigenVector ed EigenValues\n",
        "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
        "\n",
        "#Sort by descending eigenvalues.\n",
        "#This will help in deciding which eigenvector(s) can be dropped without losing significant information.\n",
        "\n",
        "# Create pairs (eigenvalue, eigenvector)\n",
        "eig_pairs = [(np.abs(eigenvalues[i]), eigenvectors[:, i]) for i in range(len(eigenvalues))]\n",
        "\n",
        "# Sort the pairs based on the eigenvalues\n",
        "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "# Scegli top-k eigenvalues\n",
        "num_components = 2\n",
        "matrix_w = np.hstack([eig_pairs[i][1].reshape(X_std.shape[1], 1) for i in range(num_components)])\n",
        "\n",
        "X_pca = X_std.dot(matrix_w)\n",
        "\n",
        "print(f\"Prima avevamo una shape sull'input di {X_std.shape}\")\n",
        "print(f\"Ora la nuova shape è {X_pca.shape}\")"
      ],
      "metadata": {
        "id": "nKWxJooMVIs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   **Calcola la Matrice delle Covarianze**: essa rappresenta le relazioni tra le variabili nel dataset.\n",
        "\n",
        "2.   **Calcola Autovalori ed Autovettori**: Questi rappresentano le caratteristiche principali della variazione nei dati. Gli autovettori rappresentano una direzione nello spazio multidimensionale delle variabili originali, mentre gli autovalori indicano quanto le dimensioni in queste direzioni vengano scalate o compressi\n",
        "\n",
        "3.   **Ordina Autovalori ed Autovettori in modo decrescente**:\n",
        "Questo passo è cruciale perché ti permette di identificare i componenti più importanti in base alla varianza spiegata\n",
        "\n",
        "\n",
        "4.   **Seleziona i Top-k Componenti**: Puoi scegliere il numero desiderato di componenti principali (autovettori) da mantenere per la riduzione della dimensionalità. Questi rappresentano le dimensioni più importanti nei dati.\n",
        "\n",
        "5. **Proietta i Dati nel Nuovo Spazio delle Feature:**\n",
        " Utilizzando gli autovettori selezionati, proietti i dati originali nello spazio delle feature ridotto attraverso una trasformazione lineare, solitamente calcolata come un prodotto scalare tra i dati e gli autovettori."
      ],
      "metadata": {
        "id": "at2Z_dQNWV4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly Detection"
      ],
      "metadata": {
        "id": "bvdIOqGqFnlw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il problema trova diverse applicazioni ad esempio rilevazione di Spam, transazioni finanziarie fraudolente, controllo della qualità industriale.\n",
        "In ciascuno di questi contesti, l'obiettivo è identificare eventi o osservazioni inusuali che richiedono attenzione o azioni speciali.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r062So-UkgZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Caratteristiche delle Anomalie**\n",
        "\n",
        " Le anomalie possono assumere diverse forme. Possono essere dei dati isolati che si discostano dai punti dati circostanti, sequenze di dati insolite, o cluster di dati che rappresentano comportamenti insoliti. La definizione di \"anomalia\" dipende dal contesto e dalle specifiche dell'applicazione.\n",
        "\n",
        " **Techniche di anomaly detection**\n",
        " Alcuni dei metodi più comuni includono la statistica descrittiva, i metodi basati su soglie, i modelli di machine learning (come Isolation Forest, One-Class SVM, Autoencoder), e l'apprendimento automatico supervisionato quando si dispone di etichette per gli outlier. Vedremo alcune techniche unsupervised per il rilevamento di anomalie.\n",
        "\n",
        "\n",
        " La valutazione dei modelli di Anomaly Detection coinvolge l'uso di metriche come la precisione, il richiamo e l'F1-score. Poiché le anomalie sono spesso rare, è importante considerare l'equilibrio tra falsi positivi e falsi negativi."
      ],
      "metadata": {
        "id": "uYLoohFvIUZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs, make_moons\n",
        "\n",
        "# Example settings\n",
        "n_samples = 300\n",
        "outliers_fraction = 0.15\n",
        "n_outliers = int(outliers_fraction * n_samples)\n",
        "n_inliers = n_samples - n_outliers\n",
        "rng = np.random.RandomState(42)\n",
        "\n",
        "matplotlib.rcParams[\"contour.negative_linestyle\"] = \"solid\"\n",
        "\n",
        "# Define datasets\n",
        "blobs_params = dict(random_state=0, n_samples=n_inliers, n_features=2)\n",
        "\n",
        "dataset_1 = make_blobs(centers=[[0, 0], [0, 0]], cluster_std=0.5, **blobs_params)[0]\n",
        "dataset_2 = make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[0.5, 0.5], **blobs_params)[0]\n",
        "dataset_3 = make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[1.5, 0.3], **blobs_params)[0]\n",
        "dataset_4 = 4.0*( make_moons(n_samples=n_samples, noise=0.05, random_state=0)[0] - np.array([0.5, 0.25]) )\n",
        "\n",
        "datasets = [dataset_1, dataset_2, dataset_3, dataset_4]\n",
        "\n",
        "plt.figure(figsize=(15, 3))\n",
        "for i, X in enumerate(datasets):\n",
        "    #add outliers\n",
        "    X = np.concatenate([X, rng.uniform(low=-6, high=6, size=(n_outliers, 2))], axis=0)\n",
        "    # O =  rng.uniform(low=-6, high=6, size=(n_outliers, 2))\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.scatter(X[:, 0], X[:, 1] )\n",
        "    # plt.scatter(O[:, 0], O[:, 1] )\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    plt.title(f'Dataset {i + 1}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "khOj5zKIKgqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ElHn8AyvTZSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs, make_moons\n",
        "\n",
        "# Example settings\n",
        "n_samples = 300\n",
        "outliers_fraction = 0.15\n",
        "n_outliers = int(outliers_fraction * n_samples)\n",
        "n_inliers = n_samples - n_outliers\n",
        "rng = np.random.RandomState(42)\n",
        "\n",
        "matplotlib.rcParams[\"contour.negative_linestyle\"] = \"solid\"\n",
        "\n",
        "# Define datasets\n",
        "blobs_params = dict(random_state=0, n_samples=n_inliers, n_features=2)\n",
        "\n",
        "dataset_1 = make_blobs(centers=[[0, 0], [0, 0]], cluster_std=0.5, **blobs_params)[0]\n",
        "dataset_2 = make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[0.5, 0.5], **blobs_params)[0]\n",
        "dataset_3 = make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[1.5, 0.3], **blobs_params)[0]\n",
        "dataset_4 = 4.0*( make_moons(n_samples=n_samples, noise=0.05, random_state=0)[0] - np.array([0.5, 0.25]) )\n",
        "\n",
        "datasets = [dataset_1, dataset_2, dataset_3, dataset_4]\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "\n",
        "#Ho un punto e voglio classificarlo come simile o non simile ai suoi vicini\n",
        "#Costruisco degli alberi decisionali per farlo e vedo quanto sono profondi\n",
        "#le anomalie hanno alberi poco profondi perchè è diverso dai suoi vicini.\n",
        "isolation_forest = IsolationForest(contamination=outliers_fraction)\n",
        "\n",
        "# One-Class SVM (Support Vector Machine)\n",
        "oc_svm = OneClassSVM(nu=outliers_fraction,  kernel=\"rbf\", gamma=0.1)  # Imposta il parametro nu, che controlla la percentuale di punti normali\n",
        "\n",
        "lof = LocalOutlierFactor(n_neighbors=35, contamination=outliers_fraction, novelty=True )  # Imposta novelty=True per il rilevamento delle anomalie\n",
        "# LOF è un valore calcolato sulla base della densità locale di un punto dato, e misura quanto un punto è significativamente diverso (e quindi è un outlier) rispetto ai suoi vicini.\n",
        "\n",
        "# Elliptic Envelope presuppone che i dati abbiano distribuzione gaussiana e guarda alla loro covarianza (crea solo delle ellissi)\n",
        "elliptic = EllipticEnvelope(contamination=outliers_fraction, random_state=42) # Imposta la percentuale di contaminazione\n",
        "\n",
        "\n",
        "anomaly_algorithms=[\n",
        "    (\"Robust covariance\",elliptic ),\n",
        "    (\"Isolation Forest\",isolation_forest ),\n",
        "    (\"One-Class SVM\",oc_svm ),\n",
        "    (\"Local Outlier Factor\",lof )\n",
        "]\n",
        "xx, yy = np.meshgrid(np.linspace(-7, 7, 150), np.linspace(-7, 7, 150))\n",
        "\n",
        "plt.figure(figsize=(len(anomaly_algorithms) * 4, len(datasets) * 4))\n",
        "plt.subplots_adjust(\n",
        "    left=0.02, right=0.98, bottom=0.01, top=0.96, wspace=0.05, hspace=0.2\n",
        ")\n",
        "\n",
        "plot_num = 1\n",
        "rng = np.random.RandomState(42)\n",
        "\n",
        "for i_dataset, X in enumerate(datasets):\n",
        "    # Add outliers\n",
        "    X = np.concatenate([X, rng.uniform(low=-6, high=6, size=(n_outliers, 2))], axis=0)\n",
        "\n",
        "    for name, algorithm in anomaly_algorithms:\n",
        "        t0 = time.time()\n",
        "        if name == \"Local Outlier Factor\" and algorithm.novelty:\n",
        "          y_pred =  algorithm.fit(X).predict(X)\n",
        "        else:\n",
        "          y_pred =  algorithm.fit_predict(X)\n",
        "\n",
        "        t1 = time.time()\n",
        "        plt.subplot(len(datasets), len(anomaly_algorithms), plot_num)\n",
        "        plt.title(name, size=18)\n",
        "\n",
        "        Z = -algorithm.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "        plt.contourf(xx, yy, Z, cmap=plt.cm.Blues, alpha=0.6)\n",
        "\n",
        "        colors = np.array([\"#377eb8\", \"#ff7f00\"])\n",
        "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[(y_pred + 1) // 2])\n",
        "\n",
        "        plt.xlim(-7, 7)\n",
        "        plt.ylim(-7, 7)\n",
        "        plt.xticks(())\n",
        "        plt.yticks(())\n",
        "        plt.text(\n",
        "            0.99,\n",
        "            0.01,\n",
        "            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n",
        "            transform=plt.gca().transAxes,\n",
        "            size=15,\n",
        "            horizontalalignment=\"right\",\n",
        "        )\n",
        "        plot_num += 1\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eGVQyIaVH2jy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}